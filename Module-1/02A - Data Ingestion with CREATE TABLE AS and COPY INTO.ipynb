{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32117314-7cea-44eb-8186-c26fc77ab399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2A - Data Ingestion with CREATE TABLE AS and COPY INTO\n",
    "In this demonstration, we'll explore ingestion data from cloud storage into Delta tables with the CREATE TABLE AS (CTAS) AND COPY INTO statements.\n",
    "\n",
    "Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "\n",
    "Use the CTAS statement with read_files() to ingest Parquet files into a Delta table.\n",
    "Use COPY INTO to incrementally load Parquet files from cloud object storage into a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a9c704-578c-4255-ab39-b52e459482db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Change the default catalog/schema\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA labuser101;\n",
    "\n",
    "\n",
    "-- View current catalog and schema\n",
    "SELECT \n",
    "  current_catalog(), \n",
    "  current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce827716-9254-42d2-a837-75c64cd6a518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "B. Explore the Data Source Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37c8b609-c3b8-4df7-8b65-2cc59121efc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We'll create a table containing historical user data from Parquet files stored in the volume\n",
    "'/Volumes/dbacademy_ecommerce/v01/raw/users-historical' within Unity Catalog.\n",
    "\n",
    "Use the LIST statement to view the files in this volume. Run the cell and review the results.\n",
    "\n",
    "Notice the files in the name column begin with part-. This shows that this volume contains multiple Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc90f7fe-d14c-44b7-aadf-0c74c5807adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/dbacademy_ecommerce/v01/raw/user_historical'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7a6c426-a81f-4d4c-bafb-3292c88d9e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query the Parquet files by path in the /Volumes/dbacademy_ecommerce/v01/raw/users-historical directory to view the raw data in tabular format to quickly preview the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ebc1ba-7a49-470a-8fa3-33c8991fe849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM parquet.`/Volumes/dbacademy_ecommerce/v01/raw/user_historical`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3c5c20b-26a0-44a2-81d5-7d58220650a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "C. Batch Data Ingestion with CTAS and read_files()\n",
    "The CREATE TABLE AS (CTAS) statement is used to create and populate tables using the results of a query. This allows you to create a table and load it with data in a single step, streamlining data ingestion workflows.\n",
    "\n",
    "Automatic Schema Inference for Parquet Files\n",
    "Apache Parquet is a columnar storage format optimized for analytical queries. It includes embedded schema metadata (e.g., column names and data types), which enables automatic schema inference when creating tables from Parquet files. This eliminates the need for manual schema definitions and simplifies the process of converting Parquet files into Delta format by leveraging the built-in schema metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c457cd-b132-47c5-b7ea-1b6e5d2dbb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "C1. CTAS with the read_files() Function\n",
    "The code in the next cell creates a table using CTAS with the read_files() function.\n",
    "\n",
    "The read_files() table-valued function (TVF) enables reading a variety of file formats and provides additional options for data ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7510286f-9404-409f-a3e8-2a1761e46091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, let's explore the documentation for read_files. Complete the following steps:\n",
    "\n",
    "a. Navigate to the read_files documentation.\n",
    "\n",
    "b. Scroll down and find the Options section. Take a moment to explore some of the features of read_files.\n",
    "\n",
    "c. In the Options section, notice the variety of options available based on the file type.\n",
    "\n",
    "d. Click on parquet and scroll through the available options.\n",
    "\n",
    "NOTE: The read_files function provides a wide range of capabilities and specific options for each file type. The previous method used to create a table only works if no additional options are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c6a5873-4649-4e1f-9485-816300873227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2.Use the read_files() function to query the same Parquet files located in /Volumes/dbacademy_ecommerce/v01/raw/users-historical. The LIMIT clause limits the amount of rows during exploration and development.\n",
    "\n",
    "The first parameter in read_files is the path to the data.\n",
    "\n",
    "The format => \"parquet\" option specifies the file format.\n",
    "\n",
    "The read_files function automatically detects the file format and infers a unified schema across all files. It also supports explicit schema definitions and schemaHints. For more details on schema inference capabilities, refer to the Schema inference documentation.\n",
    "\n",
    "NOTE: A _rescued_data column is automatically included by default to capture any data that doesn’t match the inferred schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58b305e-f9d7-43a1-bc48-f3ce89c76d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM read_files(\n",
    "  '/Volumes/dbacademy_ecommerce/v01/raw/user_historical',\n",
    "  format => 'parquet'\n",
    ")\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39689a2d-fc17-42e8-8151-317e64087229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, let's use read_files() with a CTAS statement to create the table historical_users_bronze_ctas_rf, then display the table.\n",
    "\n",
    "Notice that the Parquet files were ingested create a table (Delta by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab2b973-9c39-4245-9656-5968b67f85fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS historical_users_bronze_ctas_rf;\n",
    "\n",
    "\n",
    "-- Create the Delta table\n",
    "CREATE TABLE historical_users_bronze_ctas_rf \n",
    "SELECT * \n",
    "FROM read_files(\n",
    "        '/Volumes/dbacademy_ecommerce/v01/raw/user_historical',\n",
    "        format => 'parquet'\n",
    "      );\n",
    "\n",
    "\n",
    "-- Preview the Delta table\n",
    "SELECT * \n",
    "FROM historical_users_bronze_ctas_rf \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f87f5f23-a865-44be-a92f-9a95f4627340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the DESCRIBE TABLE EXTENDED statement to view column names, data types, and additional table metadata.\n",
    "\n",
    "Review the results and notice the following:\n",
    "\n",
    "The table was created in your schema within the course catalog dbacademy.\n",
    "\n",
    "The Type row indicates that the table is MANAGED.\n",
    "\n",
    "The Location row shows the managed cloud storage location.\n",
    "\n",
    "The Provider row specifies that the table is a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6886b764-b20b-4033-9123-800a47499c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED historical_users_bronze_ctas_rf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356b9b3d-dffb-457b-9124-3970de9a70ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Managed vs External Tables in Databricks\n",
    "Managed Tables\n",
    "Databricks manages both the data and metadata.\n",
    "Data is stored within Databricks’ managed storage.\n",
    "Dropping the table also deletes the data.\n",
    "Recommended for creating new tables.\n",
    "External Tables\n",
    "Databricks only manages the table metadata.\n",
    "Dropping the table does not delete the data.\n",
    "Supports multiple formats, including Delta Lake.\n",
    "Ideal for sharing data across platforms or using existing external data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26c047d2-07b7-48e3-af66-3404e60cf6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "D1. Ingesting Parquet Files with COPY INTO\n",
    "Using the same set of Parquet files as before, let's use COPY INTO to create our Bronze table again.\n",
    "\n",
    "We will look at two examples:\n",
    "\n",
    "Example 1: Common Schema Mismatch Error\n",
    "\n",
    "Example 2: Preemptively Handling Schema Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45094e09-8569-44d7-891b-e786058110aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Example 1: Common Schema Mismatch Error\n",
    "The cell below creates an empty table named historical_users_bronze_ci with a defined schema for only the user_id and user_first_touch_timestamp columns.\n",
    "\n",
    "However, the Parquet files in '/Volumes/dbacademy_ecommerce/v01/raw/users-historical' contain three columns:\n",
    "\n",
    "user_id\n",
    "user_first_touch_timestamp\n",
    "email\n",
    "Run the cell below and review the error. You should see the [COPY_INTO_SCHEMA_MISMATCH_WITH_TARGET_TABLE] error. This error occurs because there is a schema mismatch: the Parquet files contain 3 columns, but the target table historical_users_bronze_ci only has 2 columns.\n",
    "\n",
    "How can you handle this error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91011f4-ee6e-4a16-84dd-054a5c51b5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--------------------------------------------\n",
    "-- This cell returns an error\n",
    "--------------------------------------------\n",
    "\n",
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS historical_users_bronze_ci;\n",
    "\n",
    "\n",
    "-- Create an empty table with the specified table schema (only 2 out of the 3 columns)\n",
    "CREATE TABLE historical_users_bronze_ci (\n",
    "  user_id STRING,\n",
    "  user_first_touch_timestamp BIGINT\n",
    ");\n",
    "\n",
    "\n",
    "-- Use COPY INTO to populate Delta table\n",
    "COPY INTO historical_users_bronze_ci\n",
    "  FROM '/Volumes/dbacademy_ecommerce/v01/raw/user_historical'\n",
    "  FILEFORMAT = parquet;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9da5a43-2f0a-406b-8b03-960b9effaf82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can fix this error by adding COPY_OPTIONS with the mergeSchema = 'true' option. When set to true, this option allows the schema to evolve based on the incoming data.\n",
    "\n",
    "Run the next cell with the COPY_OPTIONS option added. You should notice that the Parquet files were successfully ingested into the table, with a total of 251,501 rows ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7b8522c-f191-42ff-a499-7cb2536684ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "COPY INTO historical_users_bronze_ci\n",
    "  FROM '/Volumes/dbacademy_ecommerce/v01/raw/user_historical'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');     -- Merge the schema of each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d3c2523-089b-4f9e-bc86-3107ca068ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Preview the data in the historical_users_bronze_ci table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda2c546-e559-4c75-bfac-5692318dd6e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM historical_users_bronze_ci\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fbaaa46-65cc-48d9-bcd2-7f54f3ca75aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Example 2: Preemptively Handling Schema Evolution\n",
    "Another way to ingest the same files into a Delta table is to start by creating an empty table named historical_users_bronze_ci_no_schema.\n",
    "\n",
    "Then, add the COPY_OPTIONS ('mergeSchema' = 'true') option to enable schema evolution for the table.\n",
    "\n",
    "Run the cell and confirm that 251,501 rows were added to the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ac8cf8-9d9d-46f9-851c-778c5ecc0b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS historical_users_bronze_ci_no_schema;\n",
    "\n",
    "\n",
    "-- Create an empty table without the specified schema\n",
    "CREATE TABLE historical_users_bronze_ci_no_schema;\n",
    "\n",
    "\n",
    "-- Use COPY INTO to populate Delta table\n",
    "COPY INTO historical_users_bronze_ci_no_schema\n",
    "  FROM '/Volumes/dbacademy_ecommerce/v01/raw/user_historical'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6eb70fb-3e39-4768-b29c-4f010f9cdb11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "D2. Idempotency (Incremental Ingestion)\n",
    "COPY INTO tracks the files it has previously ingested. If the command is run again, no additional data is ingested because the files in the source directory haven't changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd3e3205-f773-44fd-8538-01a17756e1b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's run the COPY INTO command again and check if any data is re-ingested into the table.\n",
    "\n",
    "Run the cell and view the results. Notice that the values for num_affected_rows, num_inserted_rows, and num_skipped_corrupt_files are all 0 because the data has already been ingested into the Delta table.\n",
    "\n",
    "NOTE: If new files are added to the cloud storage location, COPY INTO will only ingest those files. Using COPY INTO is a great option if you want to run a job for incremental batch ingestion from cloud storage location without re-reading files that have already been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c40aa46-6b6c-49f7-bfc4-8795f9beb078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "COPY INTO historical_users_bronze_ci_no_schema\n",
    "  FROM '/Volumes/dbacademy_ecommerce/v01/raw/user_historical'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5264973390887446,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02A - Data Ingestion with CREATE TABLE AS and COPY INTO",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
